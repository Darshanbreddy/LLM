{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8iFqk/4gHcFvgrlNU4OZw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Darshanbreddy/LLM/blob/main/Linear__Layer(1%262)%2C_GELU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clear demonstration of how input data is transformed step-by-step through a simple feedforward neural network using different activation functions (ReLU vs GELU), showing the effect of nonlinearities and linear layers on the data representation from raw input all the way to final output."
      ],
      "metadata": {
        "id": "4ph7-s_wLvfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Block 1: Compare ReLU vs GELU on the same input"
      ],
      "metadata": {
        "id": "qjSHF6AcK4NA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV6e7_YMKr_R",
        "outputId": "d3d35642-768a-4a74-e109-f36723b8b48b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "tensor([-3.0000, -2.3333, -1.6667, -1.0000, -0.3333,  0.3333,  1.0000,  1.6667,\n",
            "         2.3333,  3.0000])\n",
            "\n",
            "ReLU Output:\n",
            "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 1.0000, 1.6667, 2.3333,\n",
            "        3.0000])\n",
            "\n",
            "GELU Output:\n",
            "tensor([-0.0040, -0.0229, -0.0797, -0.1587, -0.1231,  0.2102,  0.8413,  1.5870,\n",
            "         2.3104,  2.9960])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Input tensor\n",
        "x = torch.linspace(-3, 3, steps=10)\n",
        "\n",
        "# Apply ReLU and GELU\n",
        "relu_output = F.relu(x)\n",
        "gelu_output = F.gelu(x)\n",
        "\n",
        "print(\"Input:\")\n",
        "print(x)\n",
        "print(\"\\nReLU Output:\")\n",
        "print(relu_output)\n",
        "print(\"\\nGELU Output:\")\n",
        "print(gelu_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom GELU implementation\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / 3.14159265)) * (x + 0.044715 * x**3)))\n",
        "\n",
        "# Input (batch_size=3, features=8)\n",
        "x = torch.randn(3, 8)\n",
        "\n",
        "# Define Layer 1: Linear(8 ➝ 16) + GELU\n",
        "def layer1(x):\n",
        "    w1 = torch.randn(8, 16)\n",
        "    b1 = torch.randn(16)\n",
        "    return gelu(x @ w1 + b1)\n",
        "\n",
        "# Apply Layer 1\n",
        "out1 = layer1(x)\n",
        "print(\"\\nOutput of Layer 1 (Linear + custom GELU):\")\n",
        "print(out1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW84W3CkLNmQ",
        "outputId": "ebf4e694-0d31-49db-e00b-275025334103"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output of Layer 1 (Linear + custom GELU):\n",
            "tensor([[ 1.0552e+00,  4.0823e+00, -1.8030e-02, -2.6953e-04, -1.7004e-01,\n",
            "         -0.0000e+00, -2.2586e-02, -7.3840e-03,  2.4161e+00,  1.5999e+00,\n",
            "          3.2410e+00,  2.0133e+00, -1.5462e-01,  4.4627e+00, -0.0000e+00,\n",
            "          1.0894e+01],\n",
            "        [ 1.6755e+00,  2.9511e+00, -5.5913e-02,  1.4067e+00,  3.2412e-01,\n",
            "         -2.5748e-04, -1.4914e-07,  5.4777e+00, -1.1242e-02,  6.0841e-02,\n",
            "          2.2075e-01, -1.6773e-01, -3.8090e-02,  4.8550e+00,  2.5292e-01,\n",
            "          2.2803e+00],\n",
            "        [ 1.8880e+00,  3.0105e+00, -8.9843e-02, -1.6121e-01,  2.1669e+00,\n",
            "          2.5209e+00,  1.2876e+00, -1.6372e-01,  7.9291e-01,  1.3498e+00,\n",
            "          1.0105e+00,  1.6389e+00, -1.6968e-03, -1.3546e-01,  2.2051e+00,\n",
            "         -1.4944e-01]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 3: Define Layer 2 (Linear 16 ➝ 8)"
      ],
      "metadata": {
        "id": "VZD8IgoHLRS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Layer 2: Linear(16 ➝ 8)\n",
        "def layer2(x):\n",
        "    w2 = torch.randn(16, 8)\n",
        "    b2 = torch.randn(8)\n",
        "    return x @ w2 + b2\n",
        "\n",
        "# Apply Layer 2\n",
        "out2 = layer2(out1)\n",
        "print(\"\\nOutput of Layer 2 (Linear):\")\n",
        "print(out2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSKJzydtLRzv",
        "outputId": "78663a6d-4a50-4738-e1ec-678edd04a11e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output of Layer 2 (Linear):\n",
            "tensor([[-10.2599,   2.8703,  -7.9834,   0.7555,  13.7122,  11.7981,  19.3064,\n",
            "          -4.5272],\n",
            "        [  8.5440,  -3.1209,   3.3968,   7.5250,  -3.9780,   6.6603,  19.0782,\n",
            "          11.0782],\n",
            "        [  8.8261,  -0.3243,  -1.8002,  -2.1974,   4.3282,  -1.4196,  -3.2187,\n",
            "           0.4414]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Block 4: Show Original Input and Final Output"
      ],
      "metadata": {
        "id": "ogEuMeYZLWPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nOriginal Input:\")\n",
        "print(x)\n",
        "\n",
        "print(\"\\nFinal Output after Layer 1 ➝ GELU ➝ Layer 2:\")\n",
        "print(out2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq4-trbqLWuG",
        "outputId": "70f96938-e51e-4f2b-bbfa-537171465b42"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original Input:\n",
            "tensor([[-2.3009, -0.0052, -0.1872,  2.6048, -1.2609,  0.6406, -0.9216, -1.5741],\n",
            "        [-2.3114, -0.4795,  1.6537, -0.7362, -0.0311, -1.1922, -0.6694,  0.2523],\n",
            "        [ 0.0401, -0.2795, -0.4661,  0.6647, -0.0064, -1.0065,  0.1872, -0.9176]])\n",
            "\n",
            "Final Output after Layer 1 ➝ GELU ➝ Layer 2:\n",
            "tensor([[-10.2599,   2.8703,  -7.9834,   0.7555,  13.7122,  11.7981,  19.3064,\n",
            "          -4.5272],\n",
            "        [  8.5440,  -3.1209,   3.3968,   7.5250,  -3.9780,   6.6603,  19.0782,\n",
            "          11.0782],\n",
            "        [  8.8261,  -0.3243,  -1.8002,  -2.1974,   4.3282,  -1.4196,  -3.2187,\n",
            "           0.4414]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 1: Compares ReLU and GELU activation functions on the same input to show their different nonlinear behaviors.\n",
        "\n",
        "Block 2: Defines a custom GELU function and applies it in a Linear(8→16) layer to simulate the first feedforward step.\n",
        "\n",
        "Block 3: Applies a second Linear(16→8) layer to transform the output of Layer 1 further.\n",
        "\n",
        "Block 4: Displays the original input and the final output after passing through both layers to track the full transformation."
      ],
      "metadata": {
        "id": "IXNE1fShLjd-"
      }
    }
  ]
}